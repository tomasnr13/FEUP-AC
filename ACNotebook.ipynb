{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as nm\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#DU\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "#PREP\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = pd.read_csv(\"ficheiros_competicao_dev/account.csv\")\n",
    "cards = pd.read_csv(\"ficheiros_competicao_dev/card_dev.csv\")\n",
    "clients = pd.read_csv(\"ficheiros_competicao_dev/client.csv\")\n",
    "disps = pd.read_csv(\"ficheiros_competicao_dev/disp.csv\")\n",
    "districts = pd.read_csv(\"ficheiros_competicao_dev/district.csv\")\n",
    "loans = pd.read_csv(\"ficheiros_competicao_dev/loan_dev.csv\")\n",
    "trans = pd.read_csv(\"ficheiros_competicao_dev/trans_dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate data analysis: min max avg std mean median mode boxplot barplot (variance)\n",
    "Multivariate data analysis: first last count scatterplot heatmap  (covariance matrix) parallel?\n",
    "info: data types, columns, null values, outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General statistics analysis functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Basic Column Statistics values - max, min, count, average, standard deviation, first, last\n",
    "\n",
    "def getCount(df, col): return df[col].count()\n",
    "def getMin(df, col):   return df[col].min()\n",
    "def getMax(df, col):   return df[col].max()\n",
    "def getAvg(df, col):   return df[col].mean()\n",
    "def getFirst(df):          return df.iloc[0]\n",
    "def getLast(df):           return df.iloc[-1]\n",
    "def getStd(df, col):   return df[col].std()\n",
    "\n",
    "def data_statistics(table): \n",
    "    for col in table.columns:\n",
    "        print(table[col].dtype)\n",
    "        if(table[col].dtype != \"object\"):\n",
    "            print(col)\n",
    "            # Finding Mean\n",
    "            print(\"\\nMean: \", statistics.mean(table[col]))\n",
    "            \n",
    "            # Finding Median\n",
    "            print(\"Median: \", statistics.median(table[col]))\n",
    "            \n",
    "        # Finding Single Mode\n",
    "        print(\"Single Mode: \", statistics.mode(table[col]))\n",
    "            \n",
    "        # Finding Multiple Modes\n",
    "        print(\"Mode: \", statistics.multimode(table[col]))\n",
    "\n",
    "#Get description\n",
    "def data_describe(dataset):\n",
    "    print(\"Dataset: \")\n",
    "    print(dataset)\n",
    "\n",
    "    print(\"Dataset info: \")\n",
    "    print(dataset.info())\n",
    "\n",
    "    print(\"General info about the dataset: \")\n",
    "    print(dataset.describe())\n",
    "\n",
    "    print(\"Types of every collumn of the dataset: \")\n",
    "    print(dataset.dtypes)\n",
    "\n",
    "    print(\"Check the dataset for null values: \")\n",
    "    print(dataset.isnull().sum())\n",
    "\n",
    "def getnulls(dataset):\n",
    "    return dataset.isna().any(axis=1).sum()\n",
    "\n",
    "def getoutliers(dataset, column): #not applied\n",
    "    q_low = dataset[column].quantile(0.01)\n",
    "    q_hi  = dataset[column].quantile(0.99)\n",
    "\n",
    "    filtered = dataset[(dataset[column] < q_hi) & (dataset[column] > q_low)]\n",
    "\n",
    "    return len(dataset) - len(filtered)\n",
    "\n",
    "def variance(dataset): #not applied\n",
    "    print(dataset.var())\n",
    "\n",
    "def covariance_matrix(dataset):#not applied\n",
    "    print(dataset.cov())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot-related Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(dataset):\n",
    "    sns.set_theme(style=\"ticks\")\n",
    "    sns.pairplot(dataset, hue=\"status\")\n",
    "    plt.show()\n",
    "    \n",
    "def boxplot(datasetCols, datasetList):\n",
    "   \n",
    "    fig = plt.figure(figsize =(10, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Creating axes instance\n",
    "    bp = ax.boxplot(datasetList, patch_artist = True,\n",
    "                    notch ='True', vert = 0)\n",
    "\n",
    "    colors = ['#0000FF', '#00FF00',\n",
    "            '#FFFF00', '#FF00FF']\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    # changing color and linewidth of\n",
    "    # whiskers\n",
    "    for whisker in bp['whiskers']:\n",
    "        whisker.set(color ='#8B008B',\n",
    "                    linewidth = 1.5,\n",
    "                    linestyle =\":\")\n",
    "    \n",
    "    # changing color and linewidth of\n",
    "    # caps\n",
    "    for cap in bp['caps']:\n",
    "        cap.set(color ='#8B008B',\n",
    "                linewidth = 2)\n",
    "    \n",
    "    # changing color and linewidth of\n",
    "    # medians\n",
    "    for median in bp['medians']:\n",
    "        median.set(color ='red',\n",
    "                linewidth = 3)\n",
    "    \n",
    "    # changing style of fliers\n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker ='D',\n",
    "                color ='#e7298a',\n",
    "                alpha = 0.5)\n",
    "        \n",
    "    # x-axis labels\n",
    "    ax.set_yticklabels(datasetCols)\n",
    "    \n",
    "    # Adding title\n",
    "    plt.title(\"Box Plot\")\n",
    "    \n",
    "    # Removing top axes and right axes\n",
    "    # ticks\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def barplotcount(dataset, column_name):\n",
    "    sns.countplot(x = column_name, data = dataset)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(dataset):\n",
    "    # Creating correlation matrix\n",
    "    dataset_corr = dataset.corr().abs()\n",
    "    print(dataset_corr)\n",
    "\n",
    "    sns.heatmap(dataset_corr, cmap='RdYlGn_r', linewidths=0.5, annot=True)\n",
    "    plt.yticks(rotation= 0)\n",
    "    plt.xticks(rotation=90)\n",
    "    # Display the Pharma Sector Heatmap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def du():\n",
    "    nulls = {}\n",
    "    outliers = {}\n",
    "    # account \n",
    "    # accounts = pd.read_csv(\"ficheiros_competicao_dev/account.csv\")\n",
    "    nulls['account'] = account.isna().any(axis=1).sum()\n",
    "    # # card \n",
    "    # cards = pd.read_csv(\"ficheiros_competicao_dev/card_dev.csv\")\n",
    "    nulls['card'] = cards.isna().any(axis=1).sum()\n",
    "    barplotcount(cards, \"type\")     \n",
    "    # # client \n",
    "    clients = pd.read_csv(\"ficheiros_competicao_dev/client.csv\")\n",
    "    # nulls['clients'] = clients.isna().any(axis=1).sum()\n",
    "\n",
    "    # # disp \n",
    "    # disps = pd.read_csv(\"ficheiros_competicao_dev/disp.csv\")\n",
    "    # nulls['disps'] = disps.isna().any(axis=1).sum()\n",
    "\n",
    "    # barplotcount(disps, \"type\")\n",
    "\n",
    "    # # district \n",
    "    districts = pd.read_csv(\"ficheiros_competicao_dev/district.csv\")\n",
    "    # nulls['district'] = districts.isna().any(axis=1).sum()\n",
    "    # ax =  districts.plot.bar(x=\"code \",y=\"no. of inhabitants\")\n",
    "    # plt.show()\n",
    "    # # loans \n",
    "    loans = pd.read_csv(\"ficheiros_competicao_dev/loan_dev.csv\")\n",
    "    # lbp = pd.DataFrame({'amount' : loans.amount})\n",
    "    # boxplot(lbp)\n",
    "    # nulls['loans'] = loans.isna().any(axis=1).sum()\n",
    "    barplotcount(loans, \"status\")\n",
    "\n",
    "    # trans \n",
    "    trans = pd.read_csv(\"ficheiros_competicao_dev/trans_dev.csv\")\n",
    "\n",
    "    #BoxPlot trans_dev\n",
    "    # boxplot(['amount'],[trans['amount']])\n",
    "    # boxplot(['balance'],[trans['balance']])\n",
    "\n",
    "    # #BoxPlot loan_dev\n",
    "    # boxplot(['amount'],[loans['amount']])\n",
    "    # boxplot(['duration'],[loans['duration']])\n",
    "\n",
    "    # #BoxPlot client\n",
    "    # boxplot(['birth_number'],[clients['birth_number']])\n",
    "\n",
    "    # #BoxPlot disp\n",
    "    # boxplot()\n",
    "\n",
    "    #BoxPlot district\n",
    "    # boxplot(['average salary'], [districts['average salary']])\n",
    "    # boxplot(['no. of inhabitants'], [districts['no. of inhabitants']])\n",
    "\n",
    "    # districts[['ola']] = districts[['ola']].apply(pd.to_numeric)  \n",
    "    # data_trans(districts,'no. of commited crimes \\'95', 'int64')\n",
    "\n",
    "    # boxplot(['no. of commited crimes \\'95'], [districts['no. of commited crimes \\'95']])\n",
    "\n",
    "    #BoxPlot account\n",
    "    boxplot(['date'],[account['date']])\n",
    "\n",
    "    nulls['trans'] = trans.isna().any(axis=1).sum()\n",
    "    # barplotcount(trans, \"type\")\n",
    "    # barplotcount(trans, \"operation\")\n",
    "    \n",
    "    # heatmap(dataset)\n",
    "    dscatter = loans.drop([\"loan_id\", \"account_id\", \"date\"], axis=1)\n",
    "    # scatterplot(dscatter)\n",
    "\n",
    "    # #nulls\n",
    "    print('Null counts: ')\n",
    "    print(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dshape=[]\n",
    "\n",
    "dataset = loans.rename({'date' : 'date_of_loan', 'amount': 'amount_loan','duration' : 'duration_loan', 'payments' : 'payments_loan', 'status' : 'status_loan'}, axis=1)\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "dataset = dataset.merge(account.rename({'frequency' : 'frequency_account', 'date' : 'date_of_creation'}, axis=1))\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "dataset = dataset.merge(pd.DataFrame(trans.groupby('account_id').size(), columns=['n.of trans']), left_on='account_id', right_index=True, how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['operation']=='credit in cash'].groupby('account_id').size(), columns=['op 1']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['operation']=='credit card withdrawal'].groupby('account_id').size(), columns=['op 2']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['operation']=='withdrawal in cash'].groupby('account_id').size(), columns=['op 3']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['operation']=='collection from another bank'].groupby('account_id').size(), columns=['op 4']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['operation']=='remittance to another bank'].groupby('account_id').size(), columns=['op 5']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['type']=='withdrawal'].groupby('account_id').size(), columns=['type 1']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['type']=='withdrawal in cash'].groupby('account_id').size(), columns=['type 2']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans[trans['type']=='credit'].groupby('account_id').size(), columns=['type 3']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "\n",
    "dataset = dataset.merge(pd.DataFrame(trans.groupby('account_id').amount.mean().round(2)).rename(columns={'amount':'avg trans amount'}), right_index=True, left_on='account_id', how=\"outer\")\n",
    "dataset = dataset.merge(pd.DataFrame(trans.groupby('account_id').balance.mean().round(2)).rename(columns={'balance':'avg trans balance'}), right_index=True, left_on='account_id', how=\"outer\")\n",
    "\n",
    "dataset = dataset.merge(pd.DataFrame(trans.groupby('account_id').bank.nunique()).rename(columns={'bank':'n_banks'}), right_index=True, left_on='account_id', how=\"outer\")\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "dataset = dataset.merge(districts)\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "acc_disp = pd.DataFrame(disps.groupby([\"account_id\"])[\"client_id\"].count()).rename(columns = {\"client_id\" : \"nr_clients\"})\n",
    "disps = disps.merge(acc_disp, left_on=\"account_id\", right_on=\"account_id\")\n",
    "disps = disps[disps[\"type\"] == \"OWNER\"]\n",
    "\n",
    "dataset = dataset.merge(disps)\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "dataset = dataset.merge(cards.rename({'type' : 'card type'}, axis=1), how=\"outer\", left_on=\"disp_id\", right_on='disp_id')\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "birthdates = nm.full(len(clients),0)\n",
    "sexes = nm.full(len(clients),0)\n",
    "for i in range(0,len(clients)):\n",
    "    val = float(clients.iloc[i]['birth_number'])\n",
    "    if (val % 10000 > 5000):\n",
    "        birthdates[i] = val-5000\n",
    "        sexes[i] = 0\n",
    "    else:\n",
    "        birthdates[i] = val\n",
    "        sexes[i] = 1\n",
    "\n",
    "clients['birth_date'] = birthdates\n",
    "clients['sex'] = sexes\n",
    "\n",
    "dataset = dataset.merge(clients)\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "\n",
    "# dataset = dataset.merge(disp_card_clie)\n",
    "\n",
    "# dshape.append(dataset.shape)\n",
    "\n",
    "# disps = disps.rename(columns={\"type\": \"disp type\"})\n",
    "# cards = cards.rename(columns={\"type\": \"card type\"})\n",
    "# disp_card = pd.merge(disps, cards, how=\"outer\")\n",
    "\n",
    "# disp_card_clients = pd.merge(disp_card, clients)\n",
    "\n",
    "# dataset = dataset.merge(disp_card_clients)\n",
    "\n",
    "# dshape.append(dataset.shape)\n",
    "\n",
    "dataset = dataset.set_index('loan_id')\n",
    "\n",
    "dshape.append(dataset.shape)\n",
    "\n",
    "print(dshape)\n",
    "\n",
    "print(dataset.columns)\n",
    "\n",
    "dataset.to_csv(\"prel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checknullcolumns(dataset):\n",
    "    n =  dataset.isna().sum().sum()\n",
    "    print(\"Number of null values:\" + str(n))\n",
    "    if(n>0):\n",
    "        for col in dataset.columns.values.tolist():\n",
    "            print(\"number of null rows in \" + col)\n",
    "            print(dataset[col].isna().sum())\n",
    "\n",
    "def nulls(dataset, jcolumns):\n",
    "    for col in jcolumns:\n",
    "        if(col in dataset.columns.tolist()):\n",
    "            for row in range(0,len(dataset)):\n",
    "                if dataset.iloc[row][col] is None:\n",
    "                    dataset.drop(row, axis=0, inplace=True)\n",
    "        else:\n",
    "            print('Error: column ' + col + ' does not exist in dataset')\n",
    "    dataset.fillna(0) \n",
    "    return dataset\n",
    "\n",
    "def removedups(dataset, columns):\n",
    "    for col in columns:\n",
    "        if (col in dataset.columns.tolist()):\n",
    "            dataset.drop_duplicates(subset=col)\n",
    "        else:\n",
    "            print('Error: column ' + col + ' does not exist in dataset')\n",
    "\n",
    "def outliers(dataset, column): #not fully inuse\n",
    "    q_low = dataset[column].quantile(0.01)\n",
    "    print(\"0.01 quantile for \" + column + \": \" + str(q_low))\n",
    "    q_hi  = dataset[column].quantile(0.99)\n",
    "    print(\"0.99 quantile for \" + column + \": \" + str(q_hi))\n",
    "\n",
    "    dataset = dataset[(dataset[column] < q_hi) & (dataset[column] > q_low)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(df, column, positive_value):\n",
    "    df = df.copy()\n",
    "    df[column] = df[column].apply(lambda x : 1 if x == positive_value else 0)\n",
    "    return df\n",
    "    \n",
    "def ordinal_encode(df, column, ordering):\n",
    "    df = df.copy()\n",
    "    df[column] = df[column].apply(lambda x : ordering.index(x))\n",
    "    return df\n",
    "\n",
    "# Encode column of dataframe\n",
    "def encode_column(df, column_to_encode):\n",
    "    \n",
    "    df_ = df.copy() # copy dataframe to avoid making changes to it inside the function\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(df_[column_to_encode].unique())\n",
    "    df_[column_to_encode] = label_encoder.transform(df_[column_to_encode])\n",
    "    \n",
    "    return df_\n",
    "\n",
    "def outliers(dataset, column): #not fully inuse\n",
    "    q_low = dataset[column].quantile(0.01)\n",
    "    print(\"0.01 quantile for \" + column + \": \" + str(q_low))\n",
    "    q_hi  = dataset[column].quantile(0.99)\n",
    "    print(\"0.99 quantile for \" + column + \": \" + str(q_hi))\n",
    "\n",
    "    dataset = dataset[(dataset[column] < q_hi) & (dataset[column] > q_low)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_processing(df):\n",
    "    # Ordinal encode the frequency column\n",
    "    freq_ordering = [\n",
    "        'monthly issuance',\n",
    "        'weekly issuance',\n",
    "        'issuance after transaction'\n",
    "    ]\n",
    "    df = ordinal_encode(df, 'frequency_account', ordering=freq_ordering)\n",
    "\n",
    "    return df\n",
    "\n",
    "def district_processing(df):\n",
    "    df = encode_column(df, 'name')\n",
    "    df = encode_column(df, 'region')\n",
    "    df.loc[df['no. of commited crimes \\'95']==\"?\", 'no. of commited crimes \\'95'] = pd.to_numeric( df[df['no. of commited crimes \\'95'] != '?']['no. of commited crimes \\'95'] ).median()\n",
    "    df.loc[df['no. of commited crimes \\'96 ']==\"?\", 'no. of commited crimes \\'96 '] = pd.to_numeric( df[df['no. of commited crimes \\'96 '] != '?']['no. of commited crimes \\'96 '] ).median()\n",
    "    \n",
    "    median_unemploymant_95 = (df[df['unemploymant rate \\'95'] != '?']['unemploymant rate \\'95']).astype(float).median()\n",
    "    df.loc[df['unemploymant rate \\'95']==\"?\", 'unemploymant rate \\'95'] = median_unemploymant_95\n",
    "    median_unemploymant_96 = (df[df['unemploymant rate \\'96'] != '?']['unemploymant rate \\'96']).astype(float).median()\n",
    "    df.loc[df['unemploymant rate \\'96']==\"?\", 'unemploymant rate \\'96'] = median_unemploymant_96\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['op 1', 'op 2', 'op 3', 'op 4', 'op 5', 'type 1', 'type 2', 'type 3']:\n",
    "    dataset[col] = dataset[col].fillna(0)\n",
    "\n",
    "dataset = dataset.drop(columns=['type'])\n",
    "\n",
    "dataset['has card'] = nm.where(dataset['issued'].isnull(), 0, 1)\n",
    "dataset = dataset.drop(columns=['issued', 'card type', 'card_id'])\n",
    "\n",
    "# removedups(dataset)\n",
    "dataset.drop(['disp_id', 'client_id', 'account_id', 'district_id'], inplace=True, axis=1)\n",
    "\n",
    "dataset = account_processing(dataset)\n",
    "dataset = district_processing(dataset)\n",
    "\n",
    "dataset['criminality per capita 95'] = dataset['unemploymant rate \\'95'].astype(float)/dataset['no. of inhabitants'].astype(float)\n",
    "dataset['criminality per capita 96'] = dataset['unemploymant rate \\'96'].astype(float)/dataset['no. of inhabitants'].astype(float)\n",
    "dataset.drop(columns=['no. of enterpreneurs per 1000 inhabitants',\"no. of commited crimes '96 \",\"no. of commited crimes '95\"], inplace=True)\n",
    "\n",
    "\n",
    "# Encode the label (loan status) column\n",
    "label_mapping = {-1: 1, 1: 0}\n",
    "dataset['status_loan'] = dataset['status_loan'].replace(label_mapping)\n",
    "\n",
    "dataset['age_loan'] = (dataset['date_of_loan'].astype('float') - dataset['birth_date'].astype('float'))/10000\n",
    "dataset['age_account_loan'] = (dataset['date_of_loan'].astype('float') - dataset['date_of_creation'].astype('float'))/10000\n",
    "dataset = dataset.drop(columns=['date_of_loan', 'birth_number', 'date_of_creation'])\n",
    "\n",
    "dataset = dataset.drop(columns=['name', 'region', 'sex'])\n",
    "\n",
    "print(dataset.columns)\n",
    "print(dataset.shape)\n",
    "\n",
    "dataset.to_csv(\"ligma.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding - Complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loan id dups?\n",
    "checknullcolumns(dataset)\n",
    "\n",
    "# Creating correlation matrix\n",
    "dataset_corr = dataset.corr().abs()\n",
    "print(dataset_corr)\n",
    "\n",
    "#corr for data-district\n",
    "# print(dataset.corr().abs())\n",
    "#corr for district\n",
    "\n",
    "def heatmap(dataset):\n",
    "    sns.heatmap(dataset_corr, cmap='RdYlGn_r', linewidths=0.5, annot=True)\n",
    "    plt.yticks(rotation= 0)\n",
    "    plt.xticks(rotation=90)\n",
    "    # Display the Pharma Sector Heatmap\n",
    "    plt.show()\n",
    "\n",
    "heatmap(dataset)\n",
    "covariance_matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choosing the upper triangle of the correlation matrix\n",
    "# upper_triangle = dataset_corr.where(nm.triu(nm.ones(dataset_corr.shape), k=1).astype(bool))\n",
    "\n",
    "# # Looking for columns with correlation higher than 0.95\n",
    "# collumn_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "\n",
    "# collumn_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X with a standard scaler\n",
    "scaler = StandardScaler()\n",
    "y = dataset['status_loan'].copy()\n",
    "\n",
    "X = dataset.drop('status_loan', axis=1).copy()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns= X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , train_size=0.8,random_state=123)\n",
    "y_train\n",
    "# nm.bincount(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{column: len(X[column].unique()) for column in X.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling dataset in order to create a balanced dataset\n",
    "sm = SMOTE(random_state=0)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.bincount(y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models with default parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    MLPClassifier(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"   Logistic Regression\",\n",
    "    \"Support Vector Machine\",\n",
    "    \"         Decision Tree\",\n",
    "    \"        Neural Network\",\n",
    "    \"         Random Forest\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced Dataset\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "for model, name in zip(models, model_names):\n",
    "    print(name + \": {:.4f}%\".format(model.score(X_test, y_test) * 100))\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Dataset\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, name in zip(models, model_names):\n",
    "    print(name + \": {:.4f}%\".format(model.score(X_test, y_test) * 100))\n",
    "    y_pred_1 = model.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred_1))\n",
    "    print(classification_report(y_test, y_pred_1))\n",
    "    metrics.roc_auc_score(y_test, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_pred_logistic = models[0].decision_function(X_test)\n",
    "\n",
    "\n",
    "logistic_fpr, logistic_tpr, threshold = roc_curve(y_test, y_pred_logistic)\n",
    "auc_logistic = auc(logistic_fpr, logistic_tpr)\n",
    "\n",
    "auc_logistic\n",
    "\n",
    "plt.figure(figsize=(5,5), dpi=100)\n",
    "plt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\n",
    "\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_comp = pd.read_csv(\"ficheiros_competicao_dev/card_comp.csv\",sep=\";\")\n",
    "loan_comp = pd.read_csv(\"ficheiros_competicao_dev/loan_comp.csv\",sep=\";\")\n",
    "trans_comp = pd.read_csv(\"ficheiros_competicao_dev/trans_comp.csv\",sep=\";\")\n",
    "\n",
    "dshape=[]\n",
    "\n",
    "data_comp = loan_comp.rename({'date' : 'date_of_loan', 'amount': 'amount_loan','duration' : 'duration_loan', 'payments' : 'payments_loan', 'status' : 'status_loan'}, axis=1)\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "data_comp = data_comp.merge(account.rename({'frequency' : 'frequency_account', 'date' : 'date_of_creation'}, axis=1))\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp.groupby('account_id').size(), columns=['n.of trans']), left_on='account_id', right_index=True, how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['operation']=='credit in cash'].groupby('account_id').size(), columns=['op 1']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['operation']=='credit card withdrawal'].groupby('account_id').size(), columns=['op 2']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['operation']=='withdrawal in cash'].groupby('account_id').size(), columns=['op 3']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['operation']=='collection from another bank'].groupby('account_id').size(), columns=['op 4']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['operation']=='remittance to another bank'].groupby('account_id').size(), columns=['op 5']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['type']=='withdrawal'].groupby('account_id').size(), columns=['type 1']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['type']=='withdrawal in cash'].groupby('account_id').size(), columns=['type 2']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp[trans_comp['type']=='credit'].groupby('account_id').size(), columns=['type 3']), right_index=True, left_on='account_id', how=\"outer\")\n",
    "\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp.groupby('account_id').amount.mean().round(2)).rename(columns={'amount':'avg trans amount'}), right_index=True, left_on='account_id', how=\"outer\")\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp.groupby('account_id').balance.mean().round(2)).rename(columns={'balance':'avg trans balance'}), right_index=True, left_on='account_id', how=\"outer\")\n",
    "\n",
    "data_comp = data_comp.merge(pd.DataFrame(trans_comp.groupby('account_id').bank.nunique()).rename(columns={'bank':'n_banks'}), right_index=True, left_on='account_id', how=\"outer\")\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "data_comp = data_comp.merge(districts)\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "data_comp = data_comp.merge(disps)\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "data_comp = data_comp.merge(cards_comp, how=\"outer\")\n",
    "data_comp = data_comp.rename({'type' : 'card type'}, axis=1)\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "data_comp = data_comp.merge(clients)\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "# data_comp = data_comp.set_index('loan_id')\n",
    "\n",
    "z  = data_comp[['loan_id']]\n",
    "z['loan_id'] = z['loan_id'].astype('Int32')\n",
    "\n",
    "data_comp = data_comp.drop(['loan_id'], axis=1)\n",
    "\n",
    "dshape.append(data_comp.shape)\n",
    "\n",
    "data_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_comp = account_processing(data_comp)\n",
    "\n",
    "for col in ['op 1', 'op 2', 'op 3', 'op 4', 'op 5', 'type 1', 'type 2', 'type 3']:\n",
    "    data_comp[col] = data_comp[col].fillna(0)\n",
    "\n",
    "data_comp = data_comp.drop(columns=['issued', 'card type', 'card_id'])\n",
    "\n",
    "# removedups(dataset)\n",
    "\n",
    "data_comp.drop(['disp_id', 'client_id', 'account_id', 'district_id'], inplace=True, axis=1)\n",
    "\n",
    "data_comp = district_processing(data_comp)\n",
    "# Encode the label (loan status) column\n",
    "label_mapping = {-1: 1, 1: 0}\n",
    "data_comp['status_loan'] = data_comp['status_loan'].replace(label_mapping)\n",
    "\n",
    "data_comp['age_loan'] = (data_comp['date_of_loan'].astype('float') - data_comp['birth_number'].astype('float'))/10000\n",
    "data_comp['age_account_loan'] = (data_comp['date_of_loan'].astype('float') - data_comp['date_of_creation'].astype('float'))/10000\n",
    "data_comp = data_comp.drop(columns=['date_of_loan', 'birth_number', 'date_of_creation'])\n",
    "\n",
    "data_comp = data_comp.drop(columns=['name', 'region','sex'])\n",
    "\n",
    "print(data_comp.columns)\n",
    "print(data_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kaggle = data_comp['status_loan'].copy()\n",
    "X_kaggle = data_comp.drop('status_loan', axis=1).copy()\n",
    "scaler = StandardScaler()\n",
    "X_kaggle = pd.DataFrame(scaler.fit_transform(X_kaggle), columns= X_kaggle.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kaggle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosing best prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT - Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "\n",
    "dt_parameter = {'criterion': ['gini','entropy'],\n",
    "                  'splitter': ['best', 'random'],\n",
    "                  'max_depth': [1, 2, 3, 4],\n",
    "                  'max_features': [1, 2, 3, 4, 'sqrt', 'auto','log2']}\n",
    "\n",
    "dt_search = GridSearchCV(dt_classifier,\n",
    "                            param_grid=dt_parameter,\n",
    "                            scoring='precision_weighted',\n",
    "                            cv=5)\n",
    "\n",
    "dt_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(dt_search.best_score_))\n",
    "print('Best parameters: {}'.format(dt_search.best_params_))\n",
    "\n",
    "scores.append({\n",
    "        'model': 'Decision Trees',\n",
    "        'best_score': dt_search.best_score_,\n",
    "        'best_params': dt_search.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(criterion='gini', max_depth=4, max_features=1, splitter='best')\n",
    "dt_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(random_state=0)\n",
    "\n",
    "\n",
    "svm_parameters = [{'kernel': ['rbf', 'linear','sigmoid'], \n",
    "                       'gamma': ['auto','scale', 1e-3], \n",
    "                       'C': [0.01, 0.1, 1],\n",
    "                       'tol':[1e-4, 1e-3]}]\n",
    "\n",
    "\n",
    "svm_search = GridSearchCV(svm_classifier,\n",
    "                            param_grid=svm_parameters,\n",
    "                            scoring='precision_weighted',\n",
    "                            n_jobs=None,\n",
    "                            cv=5)\n",
    "\n",
    "\n",
    "svm_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(svm_search.best_score_))\n",
    "print('Best parameters: {}'.format(svm_search.best_params_))\n",
    "\n",
    "scores.append({\n",
    "        'model': 'SVM',\n",
    "        'best_score': svm_search.best_score_,\n",
    "        'best_params': svm_search.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(probability=True, C=1, gamma='auto', kernel='linear', tol=0.0001)\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier =  LogisticRegression(multi_class='auto', max_iter=300)\n",
    "\n",
    "lr_parameter = {'solver': ['lbfgs', 'liblinear'],\n",
    "                'C': nm.geomspace(1e-5, 1e5, num=20)}\n",
    "\n",
    "lr_search = GridSearchCV(lr_classifier,\n",
    "                            param_grid=lr_parameter,\n",
    "                            scoring='precision_weighted',\n",
    "                            cv=5)\n",
    "\n",
    "lr_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(lr_search.best_score_))\n",
    "print('Best parameters: {}'.format(lr_search.best_params_))\n",
    "\n",
    "\n",
    "scores.append({\n",
    "        'model': 'Logistic Regression',\n",
    "        'best_score': lr_search.best_score_,\n",
    "        'best_params': lr_search.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(multi_class='auto', solver='lbfgs', C=0.1623776739188721, max_iter=300)\n",
    "lr_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier =  MLPClassifier(max_iter=100)\n",
    "\n",
    "mlp_parameter = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "                'activation': ['tanh', 'relu'],\n",
    "                'solver': ['sgd', 'adam'],\n",
    "                'alpha': [0.0001, 0.05],\n",
    "                'learning_rate': ['constant','adaptive']}\n",
    "\n",
    "mlp_search = GridSearchCV(mlp_classifier,\n",
    "                            param_grid=mlp_parameter,\n",
    "                            scoring='precision_weighted',\n",
    "                            cv=5)\n",
    "\n",
    "mlp_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(mlp_search.best_score_))\n",
    "print('Best parameters: {}'.format(mlp_search.best_params_))\n",
    "\n",
    "\n",
    "scores.append({\n",
    "        'model': 'MLP',\n",
    "        'best_score': mlp_search.best_score_,\n",
    "        'best_params': mlp_search.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(max_iter=100, activation='relu', alpha=0.0001, hidden_layer_sizes=(50, 100, 50),learning_rate='constant', solver='adam')\n",
    "mlp_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_parameter = {'n_estimators': [1,5,10]}\n",
    "\n",
    "rf_search = GridSearchCV(rf_classifier,\n",
    "                            param_grid=rf_parameter,\n",
    "                            scoring='precision_weighted',\n",
    "                            cv=5)\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(rf_search.best_score_))\n",
    "print('Best parameters: {}'.format(rf_search.best_params_))\n",
    "\n",
    "\n",
    "scores.append({\n",
    "        'model': 'Random Forest',\n",
    "        'best_score': rf_search.best_score_,\n",
    "        'best_params': rf_search.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=5)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table comparing scores for all algorithms and best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp_classifier.predict_proba(X_kaggle)\n",
    "print(predictions)\n",
    "# Dataset with two collumns, one for each loan id and one with the prediction\n",
    "\n",
    "pred_To_Class_map = pd.DataFrame(predictions, columns=mlp_classifier.classes_)\n",
    "pred_To_Class_map['Id'] = z\n",
    "predictions_df = pred_To_Class_map[['Id', 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.set_index('Id').rename(columns={1: 'Predicted'})\n",
    "predictions_df.to_csv('pred2.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "65a440aeac0c89e2af7569e0aa53b64434c4b69eb6285e2b0d174d9bca190d54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
